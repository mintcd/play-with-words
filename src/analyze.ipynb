{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NInWPV9KYuXx"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import threading\n",
        "import re\n",
        "import numpy as np\n",
        "from functools import reduce\n",
        "from multiprocessing import Pool\n",
        "from itertools import combinations\n",
        "from time import time\n",
        "import zipfile\n",
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "import csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def read_json_from_zip(zip_file_path, json_file_name):\n",
        "    \"\"\"\n",
        "    Function to read a JSON file from a ZIP archive.\n",
        "\n",
        "    Parameters:\n",
        "        zip_file_path (str): Path to the ZIP archive.\n",
        "        json_file_name (str): Name of the JSON file within the ZIP archive.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing the JSON data.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Open the zip file\n",
        "        with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "            # Extract the JSON file from the zip archive\n",
        "            with zip_ref.open(json_file_name) as json_file:\n",
        "                # Read the JSON data\n",
        "                json_data = json.load(json_file)\n",
        "        return json_data\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading JSON from ZIP: {e}\")\n",
        "        return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def split_words(text):\n",
        "    pattern = r\"\\b\\w+(?:'\\w+)?\\b|\\w+\"\n",
        "    return re.findall(pattern, text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "GuSs2EcaKFn0"
      },
      "outputs": [],
      "source": [
        "def print_key_structure(d, indent=0):\n",
        "    for key, value in d.items():\n",
        "        print('  ' * indent + str(key))\n",
        "        if isinstance(value, dict):\n",
        "            print_key_structure(value, indent + 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_maximal_complete_graphs(edges):\n",
        "    nodes = set()\n",
        "    for edge in edges:\n",
        "        nodes.update(edge)\n",
        "    nodes = sorted(nodes)\n",
        "\n",
        "    adjacency_dict = {node: set() for node in nodes}\n",
        "    for edge in edges:\n",
        "        node1, node2 = edge\n",
        "        adjacency_dict[node1].add(node2)\n",
        "        adjacency_dict[node2].add(node1)\n",
        "\n",
        "    visited = set()\n",
        "    maximal_complete_graphs = []\n",
        "\n",
        "    for node in nodes:\n",
        "        if node not in visited:\n",
        "            queue = [node]\n",
        "            component = set()\n",
        "            while queue:\n",
        "                current = queue.pop(0)\n",
        "                visited.add(current)\n",
        "                component.add(current)\n",
        "                neighbors = adjacency_dict[current]\n",
        "                for neighbor in neighbors:\n",
        "                    if neighbor not in visited:\n",
        "                        queue.append(neighbor)\n",
        "            maximal_complete_graphs.append(component)\n",
        "\n",
        "    return maximal_complete_graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_random_cycle(G, cycles=None, numbers=1000):\n",
        "  if cycles is None:\n",
        "      cycles = []\n",
        "\n",
        "  nodes = list(G.nodes())\n",
        "  for step in range(numbers):\n",
        "      idx = np.random.randint(len(nodes))\n",
        "      try:\n",
        "          cycle = nx.find_cycle(G, source=nodes[idx])\n",
        "          cycles.append(cycle)\n",
        "      except nx.NetworkXNoCycle:\n",
        "          pass\n",
        "  unique_cycles = []\n",
        "  for sublist in cycles:\n",
        "      if sublist not in unique_cycles:\n",
        "          unique_cycles.append(sublist)\n",
        "  return unique_cycles"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Wordup Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "65eRujhDKdX7"
      },
      "outputs": [],
      "source": [
        "def lookup(word, wordup):\n",
        "  for ele in wordup:\n",
        "    if ele['root'] == word.lower(): return ele\n",
        "  return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_all_words(wordup):\n",
        "  words = set()\n",
        "  for word_def in wordup:\n",
        "    words.add(word_def['root'])\n",
        "    for sense in word_def['senses']:\n",
        "      words.update([ele.lower() for ele in split_words(sense['de'])])\n",
        "  return words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_dependencies(wordup, lemmatize=True):\n",
        "  depd = dict()\n",
        "  for word_def in wordup:\n",
        "      depd[word_def['root'].lower()] = set()\n",
        "      for sense in word_def['senses']:\n",
        "        for ele in re.findall(r'\\b\\w+\\b', sense['de']):\n",
        "            depd[word_def['root'].lower()].add(ele.lower())\n",
        "  return depd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_dependants(word_list, wordup):\n",
        "  result = set()\n",
        "  for word_def in wordup:\n",
        "    if word_def['root'] in word_list:\n",
        "      for sense in word_def['senses']:\n",
        "        result.update([ele.lower() for ele in re.findall(r'\\b\\w+\\b', sense['de']) if ele.isalpha()])\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_dependers(word_list, wordup):\n",
        "  result = set()\n",
        "  for word_def in wordup:\n",
        "    for sense in word_def['senses']:\n",
        "      for ele in re.findall(r'\\b\\w+\\b', sense['de']):\n",
        "        if ele.lower() in word_list:\n",
        "          result.add(word_def['root'])\n",
        "  return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_minimal_wordlist(word_list, wordup):\n",
        "  result = set()\n",
        "  iter = 0\n",
        "\n",
        "  print(f\"Original list length: {len(word_list)}\")\n",
        "\n",
        "  while len(result) <= len(word_list) and iter < 20:\n",
        "    result = get_dependants(word_list, wordup)\n",
        "    iter += 1\n",
        "    print(f\"Reduced list length after {iter+1} iteration(s): {len(word_list)}\")\n",
        "    if word_list == result: \n",
        "      print(f\"Minimalization converges!\")\n",
        "      break\n",
        "    word_list = result \n",
        "    \n",
        "  \n",
        "  if len(result) > len(word_list):\n",
        "    print(\"Dependant list is longer than last list!\")\n",
        "\n",
        "\n",
        "  return sorted(list(result))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Morpheme Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_morpheme_dict():\n",
        "  path = \"D:\\Projects\\play-with-words\\morphemes\\lookup.csv\"\n",
        "  morpheme_dict = dict()\n",
        "  with open(path, 'r', newline='', encoding='utf-8') as csv_file:\n",
        "      csv_reader = csv.reader(csv_file)\n",
        "      next(csv_reader)\n",
        "      for row in csv_reader:\n",
        "          morpheme_dict[row[0]] = row[1].split(\" \")\n",
        "  return morpheme_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_affix_dict(param='pre' or 'suf' or all):\n",
        "  if param == 'pre': path = \"D:\\Projects\\play-with-words\\morphemes\\prefixes.csv\"\n",
        "  elif param == 'suf':  path = \"D:\\Projects\\play-with-words\\morphemes\\suffixes.csv\"\n",
        "  else: \n",
        "    raise TypeError(\"Parameter must be 'pre' or 'suf'\")\n",
        "\n",
        "  result = set()\n",
        "  with open(path, 'r', newline='', encoding='utf-8') as csv_file:\n",
        "      csv_reader = csv.reader(csv_file)\n",
        "      next(csv_reader)\n",
        "      for row in csv_reader:\n",
        "          result.add(row[0])\n",
        "  return list(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_root_dict():\n",
        "  morpheme_dict = get_morpheme_dict()\n",
        "  root_dict = dict()\n",
        "\n",
        "  for word, morphemes in morpheme_dict.items():\n",
        "    if len(morphemes) == 1:\n",
        "      root_dict[word] = morphemes[0]\n",
        "      continue\n",
        "    \n",
        "    found = False\n",
        "\n",
        "    for morpheme in morphemes:\n",
        "      if '##' not in morpheme:\n",
        "        root_dict[word] = [morpheme]\n",
        "        found = True\n",
        "    \n",
        "    if not found:\n",
        "      \"\"\" Take all prefixes as roots \"\"\"\n",
        "      root_dict[word] = [morpheme.replace(\"##\", \"\") for morpheme in morphemes if re.match(r'.*##$', morpheme)]\n",
        "      \n",
        "  return root_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_minimal_root_list(minimal_word_list, morpheme_dict):\n",
        "  minimal_root_list = set()\n",
        "  for word in minimal_word_list:\n",
        "    if word in root_dict.keys():\n",
        "      minimal_root_list.update(root_dict[word])\n",
        "    else:\n",
        "      minimal_root_list.add(word)\n",
        "      \n",
        "  return sorted(list(minimal_root_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "wordup = read_json_from_zip(\"D:\\Projects\\play-with-words\\wordup_processed.zip\", \"wordup_processed.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "34261"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "words = get_all_words(wordup)\n",
        "len(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original list length: 24287\n",
            "Reduced list length after 2 iteration(s): 24287\n",
            "Reduced list length after 3 iteration(s): 19638\n",
            "Reduced list length after 4 iteration(s): 14560\n",
            "Reduced list length after 5 iteration(s): 13528\n",
            "Reduced list length after 6 iteration(s): 13296\n",
            "Reduced list length after 7 iteration(s): 13257\n",
            "Reduced list length after 8 iteration(s): 13240\n",
            "Reduced list length after 9 iteration(s): 13236\n",
            "Minimalization converges!\n"
          ]
        }
      ],
      "source": [
        "minimal_word_list = get_minimal_wordlist([word_def['root'] for word_def in wordup], wordup)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "morpheme_dict = get_morpheme_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "prefixes = get_affix_dict('pre')\n",
        "suffixes = get_affix_dict('suf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "root_dict = get_root_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "minimal_root_list = get_minimal_root_list(minimal_word_list, morpheme_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "7013"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(minimal_root_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "'techno' in minimal_root_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(minimal_root_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "word_set = set(words)\n",
        "word_set.update(minimal_root_list)\n",
        "wordup_roots = set(word_def['root'] for word_def in wordup)\n",
        "words_to_scrape = word_set - wordup_roots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"D:/Projects/play-with-words/to-scrape.txt\", \"w\") as txt_file:\n",
        "    for word in words_to_scrape:\n",
        "        txt_file.write(word + \"\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
