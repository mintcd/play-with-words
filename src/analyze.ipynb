{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "NInWPV9KYuXx"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import threading\n",
        "import re\n",
        "import numpy as np\n",
        "from functools import reduce\n",
        "from multiprocessing import Pool\n",
        "from itertools import combinations\n",
        "from time import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "drZEjN01Mvai"
      },
      "outputs": [],
      "source": [
        "wordup = json.load(open(\"/content/drive/MyDrive/Datasets/wordup/wordup_processed.json\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QjfPaOoAKD5q"
      },
      "source": [
        "### Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuSs2EcaKFn0"
      },
      "outputs": [],
      "source": [
        "def print_key_structure(d, indent=0):\n",
        "    for key, value in d.items():\n",
        "        print('  ' * indent + str(key))\n",
        "        if isinstance(value, dict):\n",
        "            print_key_structure(value, indent + 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65eRujhDKdX7"
      },
      "outputs": [],
      "source": [
        "def lookup(word, wordup):\n",
        "  for ele in wordup:\n",
        "    if ele['root'] == word.lower(): return ele\n",
        "  return None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Jcq-xB5O6aI"
      },
      "outputs": [],
      "source": [
        "def find_maximal_complete_graphs(edges):\n",
        "    nodes = set()\n",
        "    for edge in edges:\n",
        "        nodes.update(edge)\n",
        "    nodes = sorted(nodes)\n",
        "\n",
        "    adjacency_dict = {node: set() for node in nodes}\n",
        "    for edge in edges:\n",
        "        node1, node2 = edge\n",
        "        adjacency_dict[node1].add(node2)\n",
        "        adjacency_dict[node2].add(node1)\n",
        "\n",
        "    visited = set()\n",
        "    maximal_complete_graphs = []\n",
        "\n",
        "    for node in nodes:\n",
        "        if node not in visited:\n",
        "            queue = [node]\n",
        "            component = set()\n",
        "            while queue:\n",
        "                current = queue.pop(0)\n",
        "                visited.add(current)\n",
        "                component.add(current)\n",
        "                neighbors = adjacency_dict[current]\n",
        "                for neighbor in neighbors:\n",
        "                    if neighbor not in visited:\n",
        "                        queue.append(neighbor)\n",
        "            maximal_complete_graphs.append(component)\n",
        "\n",
        "    return maximal_complete_graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g3RWzYvKcv6v"
      },
      "outputs": [],
      "source": [
        "def dict_lemmatize(wordup):\n",
        "  traversed = dict()\n",
        "  wordup = sorted(wordup, key= lambda word: len(word['root']))\n",
        "  formations = dict()\n",
        "\n",
        "  for word in wordup:\n",
        "    traversed[word['root']] = False\n",
        "  length = len(wordup)\n",
        "  for i in range(length):\n",
        "    if traversed[wordup[i]['root']] == True: continue\n",
        "\n",
        "    traversed[wordup[i]['root']] = True\n",
        "    formations[wordup[i]['root']] = set()\n",
        "\n",
        "    for j in range(i+1,length):\n",
        "      if traversed[wordup[j]['root']] == True: continue\n",
        "      if wordup[i]['root'] in wordup[j]['root']:\n",
        "        formations[wordup[i]['root']].add(wordup[j]['root'])\n",
        "        traversed[wordup[j]['root']] = True\n",
        "  return formations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "moHLD-gXeRL_"
      },
      "outputs": [],
      "source": [
        "formations = dict_lemmatize(wordup_long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-STWKrrZHF_W"
      },
      "outputs": [],
      "source": [
        "def get_pairs(wordup_processed):\n",
        "  pairs = set()\n",
        "  for word in wordup_processed:\n",
        "    for comp in word['comparisons'].keys():\n",
        "      pair = frozenset({word['root'].lower(), comp.lower()})\n",
        "      comp_def = lookup(comp.lower(), wordup_processed)\n",
        "      if comp_def and pair not in pairs and word['root'].lower() in [comp_comp.lower() for comp_comp in comp_def['comparisons']]:\n",
        "        pairs.add(pair)\n",
        "  return list(pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dvz-JhvQgeh"
      },
      "outputs": [],
      "source": [
        "def find_maximal_complete_graphs(edges):\n",
        "    nodes = set()\n",
        "    for edge in edges:\n",
        "        nodes.update(edge)\n",
        "    nodes = sorted(nodes)\n",
        "\n",
        "    adjacency_dict = {node: set() for node in nodes}\n",
        "    for edge in edges:\n",
        "        node1, node2 = edge\n",
        "        adjacency_dict[node1].add(node2)\n",
        "        adjacency_dict[node2].add(node1)\n",
        "\n",
        "    maximal_complete_graphs = []\n",
        "    visited = set()\n",
        "\n",
        "    def is_complete(nodes):\n",
        "        for pair in combinations(nodes, 2):\n",
        "            if pair[1] not in adjacency_dict[pair[0]]:\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    for start_node in nodes:\n",
        "        if start_node not in visited:\n",
        "            stack = [start_node]\n",
        "            component = set()\n",
        "            while stack:\n",
        "                node = stack.pop()\n",
        "                if node not in visited:\n",
        "                    visited.add(node)\n",
        "                    component.add(node)\n",
        "                    stack.extend(adjacency_dict[node])\n",
        "            if is_complete(component):\n",
        "                maximal_complete_graphs.append(component)\n",
        "\n",
        "    return maximal_complete_graphs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOtMPNWUnvNP"
      },
      "outputs": [],
      "source": [
        "def analyze_word(word):\n",
        "    text = Text(word, hint_language_code='en')\n",
        "    morphemes = list(text.morphemes)\n",
        "    return word, morphemes\n",
        "\n",
        "num_processes = 8\n",
        "\n",
        "with Pool(num_processes) as pool:\n",
        "    results = pool.map(analyze_word, word_list)\n",
        "\n",
        "word_morphemes_dict = {word: morphemes for word, morphemes in results}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7t-HkZw-hAPM"
      },
      "outputs": [],
      "source": [
        "# Function to lemmatize a single word\n",
        "def lemmatize_word(word_list):\n",
        "    doc = nlp(word_list)\n",
        "    return {token.text: token.lemma_ for token in doc}\n",
        "\n",
        "# Load English tokenizer, tagger, parser, NER, and word vectors\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
        "\n",
        "# Number of processes to use (adjust according to your CPU)\n",
        "num_processes = 8\n",
        "\n",
        "# Flatten the word list\n",
        "chunk_size = len(word_list) // num_processes\n",
        "word_chunks = [\" \".join(word_list[i:i+chunk_size]) for i in range(0, len(word_list), chunk_size)]\n",
        "\n",
        "# Initialize multiprocessing pool\n",
        "with Pool(num_processes) as pool:\n",
        "    # Lemmatize words in parallel\n",
        "    lemmas = pool.map(lemmatize_word, word_chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xt43HG7Bkp5w"
      },
      "outputs": [],
      "source": [
        "lemma_dict = dict()\n",
        "for lemma in lemmas:\n",
        "  lemma_dict.update(lemma)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgAEXBcXnnHj"
      },
      "outputs": [],
      "source": [
        "depd = dict()\n",
        "for word_def in wordup:\n",
        "  if word_def['currentWord']['wordRoot'].lower() in lemma_dict.keys():\n",
        "    depd[lemma_dict[word_def['currentWord']['wordRoot'].lower()]] = set()\n",
        "    for sense in word_def['senses']:\n",
        "      for ele in re.findall(r'\\b\\w+\\b', sense['de']):\n",
        "        if ele.lower() in lemma_dict.keys():\n",
        "          word = lemma_dict[ele.lower()]\n",
        "          depd[lemma_dict[word_def['currentWord']['wordRoot'].lower()]].add(word)\n",
        "        else:\n",
        "          depd[lemma_dict[word_def['currentWord']['wordRoot'].lower()]].add(ele.lower())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qSZdNc6mlL4l"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/drive/MyDrive/Datasets/dependencies.json\", \"r\") as json_file:\n",
        "  depd = json.load(json_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpuUnoQi2XsP"
      },
      "outputs": [],
      "source": [
        "import networkx as nx\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create Directed Graph\n",
        "G=nx.DiGraph()\n",
        "\n",
        "# Add a list of nodes:\n",
        "G.add_nodes_from(list(depd.keys()))\n",
        "\n",
        "# Add a list of edges:\n",
        "G.add_edges_from([(key, dependant) for key in depd.keys() for dependant in depd[key]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p3yGkt5l6tqX"
      },
      "outputs": [],
      "source": [
        "def find_random_cycle(G, cycles=None, numbers=1000):\n",
        "  if cycles is None:\n",
        "      cycles = []\n",
        "\n",
        "  nodes = list(G.nodes())\n",
        "  for step in range(numbers):\n",
        "      idx = np.random.randint(len(nodes))\n",
        "      try:\n",
        "          cycle = nx.find_cycle(G, source=nodes[idx])\n",
        "          cycles.append(cycle)\n",
        "      except nx.NetworkXNoCycle:\n",
        "          pass\n",
        "  unique_cycles = []\n",
        "  for sublist in cycles:\n",
        "      if sublist not in unique_cycles:\n",
        "          unique_cycles.append(sublist)\n",
        "  return unique_cycles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTvYkYBe84NW"
      },
      "outputs": [],
      "source": [
        "cycles = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSLpUb2D4HvB",
        "outputId": "fc11ea9b-c0c3-4f5d-8e37-096790c3029c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Censure', 'is', 'a', 'formal', 'expression', 'of', 'disapproval', 'or', 'criticism', 'usually', 'by', 'an', 'official', 'organization', 'or', 'authority', 'figure', 'It', 'is', 'applicable', 'when', 'someone', 'has', 'done', 'something', 'wrong', 'or', 'unethical', 'and', 'needs', 'to', 'be', 'held', 'accountable', 'for', 'their', 'actions', 'in', 'a', 'professional', 'or', 'political', 'setting', 'In', 'simpler', 'terms', 'censure', 'is', 'a', 'way', 'to', 'express', 'disapproval', 'or', 'criticism', 'of', \"someone's\", 'actions', 'while', 'recrimination', 'is', 'a', 'way', 'to', 'shift', 'blame', 'onto', 'someone', 'else', 'instead', 'of', 'taking', 'responsibility', 'for', \"one's\", 'own', 'actions']\n"
          ]
        }
      ],
      "source": [
        "def split_words(text):\n",
        "    pattern = r\"\\b\\w+(?:'\\w+)?\\b|\\w+\"\n",
        "    return re.findall(pattern, text)\n",
        "\n",
        "# Example usage:\n",
        "text = \"Censure is a formal expression of disapproval or criticism, usually by an official organization or authority figure. It is applicable when someone has done something wrong or unethical and needs to be held accountable for their actions in a professional or political setting.\\n\\nIn simpler terms, censure is a way to express disapproval or criticism of someone's actions, while recrimination is a way to shift blame onto someone else instead of taking responsibility for one's own actions.\"\n",
        "words = split_words(text)\n",
        "print(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0Kikif8lVei",
        "outputId": "2227cb6b-0e4a-4005-b7f4-960247277c9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Text:\n",
            "dissection\n",
            "\n",
            "Stemmed Text:\n",
            "dissect\n"
          ]
        }
      ],
      "source": [
        "# Sample text for stemming\n",
        "text = \"dissection\"\n",
        "\n",
        "# Tokenize the text\n",
        "words = word_tokenize(text)\n",
        "\n",
        "# Initialize the Porter Stemmer\n",
        "porter_stemmer = PorterStemmer()\n",
        "\n",
        "# Apply stemming to each word\n",
        "stemmed_words = [porter_stemmer.stem(word) for word in words]\n",
        "\n",
        "# Print the original text and the stemmed text\n",
        "print(\"Original Text:\")\n",
        "print(text)\n",
        "print(\"\\nStemmed Text:\")\n",
        "print(\" \".join(stemmed_words))\n",
        "s"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
